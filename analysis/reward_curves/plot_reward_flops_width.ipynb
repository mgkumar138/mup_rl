{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3694eccf",
   "metadata": {},
   "source": [
    "### Simple script to plot reward vs flops for all widths with specific depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad32426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys available: dict_keys(['cartpole_omegaconf_bs', 'ppc_omegaconf_bs', 'ppc_megasweep', 'ant_maze', 'ant_u_maze', 'ant_u4_maze', 'ant_hardest_maze', 'humanoid', 'humanoid_u_maze', 'humanoid_u_maze_long', 'humanoid_big_maze', 'ppc', 'cartpole', 'g1rough_updated', 'humanoid_orthog', 'humanoid_u_maze_orthog', 'humanoid_omegasweep', 'humanoid_u_maze_omegasweep', 'humanoid_u_maze_lrtrans', 'cartpole_10seeds', 'panda_10seeds', 'cartpolesparse_10seeds', 'cheetah_10seeds', 'cheetah_10seeds_26', 'swimmer_10seeds', 'cartpole_omegasweep', 'ppc_omegasweep', 'acrobot_10seeds', 'acrobot_10seeds_26', 'acrobotsparse_10seeds', 'acrobotsparse_10seeds_26', 'cartpole_layernorm', 'ppc_layernorm'])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option(\"mode.copy_on_write\", True)  # to make pandas shut up about making columns\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from expt_configs import config_dict, bs_dict, parameterization_dict, dashes_dict\n",
    "\n",
    "print(f\"keys available: {config_dict.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5dad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose experiment data\n",
    "config_name = 'acrobot_10seeds'\n",
    "plot_dir = \"figs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load variables and csvs\n",
    "config = config_dict[config_name]\n",
    "output_dir = config['output_dir']\n",
    "env_name = config['env_name']\n",
    "obs_dim = config['obs_dim']\n",
    "action_dim = config['action_dim']\n",
    "num_envs = config['num_envs']\n",
    "unroll_length = config['unroll_length']\n",
    "num_minibatches = config['num_minibatches']\n",
    "update_epochs = config['update_epochs']\n",
    "num_timesteps = config['num_timesteps']\n",
    "minibatch_size = config['minibatch_size']\n",
    "base_layer_size = config['base_layer_size']\n",
    "\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "fig_dir = os.path.join(plot_dir, config_name)\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "print(f\"Made directory {fig_dir}\")\n",
    "\n",
    "\n",
    "csv_files = glob.glob(os.path.join(output_dir, \"*.csv\"))\n",
    "print(f\"Found {len(csv_files)} csv files\")\n",
    "\n",
    "dfs = []\n",
    "for csv_file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {csv_file}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f3279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and clean csvs\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "combined_df = combined_df.dropna(subset=[\"eval/episode_reward\", \"num_timesteps\"]) \n",
    "\n",
    "for double_check_col in [\"num_minibatches\", \"unroll_length\", \"batch_size\", \"num_timesteps\", \"num_evals\", \"step\"]:\n",
    "    print(f\"Length before validating {double_check_col}: {len(combined_df)}\")\n",
    "    combined_df = combined_df[~((combined_df[double_check_col].apply(type) != int) & (combined_df[double_check_col].apply(type) != float))]\n",
    "    print(f\"Length after validating: {len(combined_df)}\")\n",
    "\n",
    "# turn things into ints\n",
    "combined_df[\"num_minibatches\"] = combined_df[\"num_minibatches\"].astype(int)\n",
    "combined_df[\"unroll_length\"] = combined_df[\"unroll_length\"].astype(int)\n",
    "combined_df[\"batch_size\"] = combined_df[\"batch_size\"].astype(int)\n",
    "combined_df[\"num_timesteps\"] = combined_df[\"num_timesteps\"].astype(int)\n",
    "combined_df[\"num_evals\"] = combined_df[\"num_evals\"].astype(int)\n",
    "combined_df[\"step\"] = combined_df[\"step\"].astype(int)\n",
    "combined_df[\"eval/episode_reward\"] = combined_df[\"eval/episode_reward\"].astype(float)\n",
    "\n",
    "\n",
    "# total frames calculation TODO: you need the calculation for different timesteps...\n",
    "# TODO: consider if action repeat should be included in total frames. either way it doesn't count for a second forward pass so we exclude it from optimization calculations and hence FLOP calcs\n",
    "env_step_per_training_step = combined_df[\"batch_size\"] * combined_df[\"num_minibatches\"] * combined_df[\"unroll_length\"]\n",
    "num_training_steps_per_epoch = np.ceil(combined_df[\"num_timesteps\"] / ((combined_df[\"num_evals\"] - 1) * env_step_per_training_step)).astype(int)\n",
    "actual_num_environment_steps = num_training_steps_per_epoch * env_step_per_training_step\n",
    "actual_num_seen_steps_per_epoch = combined_df[\"num_updates_per_batch\"] * actual_num_environment_steps\n",
    "combined_df[\"current_epoch\"] = (combined_df['step'] / actual_num_environment_steps).astype(int)\n",
    "combined_df[\"total_env_frames\"] = actual_num_environment_steps.astype(int) * combined_df[\"current_epoch\"]\n",
    "combined_df[\"total_frames\"] = actual_num_seen_steps_per_epoch.astype(int) * combined_df[\"current_epoch\"]\n",
    "\n",
    "# get flops\n",
    "# we need to calculate FLOPs for the models\n",
    "\n",
    "def parameter_count_mup_brax(obs_dim, action_dim, layer_size, num_hidden_layers):\n",
    "    actor_param_count = (obs_dim * layer_size) + (layer_size * action_dim * 2) + (\n",
    "        (num_hidden_layers - 1) * layer_size**2\n",
    "    )\n",
    "    critic_param_count = (obs_dim * layer_size) + layer_size + (\n",
    "        (num_hidden_layers - 1) * layer_size**2\n",
    "    )\n",
    "\n",
    "    total_param_count = actor_param_count + critic_param_count\n",
    "\n",
    "    return total_param_count\n",
    "\n",
    "\"\"\"\n",
    "flop count for brax ppo:\n",
    "FLOPs for forward pass of MLP model with batch size 1: (2 * param count) for linear layers + (2L + 1)N for multiplication scaling\n",
    "FLOPs for forward pass of resnet model with batch size 1: (2 * param count) for linear layers + (2L + 1)N for multiplication scaling + 2L for residual connections\n",
    "1 minibatch optimization step: 1 forward pass + 2 forward passes (backward) = 3 forward passes\n",
    "\n",
    "total flops: 3 * forward pass flops * batch size * num minibatches * unroll length * num updates per batch * num training steps per epoch * current epoch\n",
    "\n",
    "\"\"\"\n",
    "combined_df['total_param_count'] = parameter_count_mup_brax(\n",
    "            obs_dim, action_dim, combined_df['layer_size'], combined_df['num_layers']\n",
    "        )\n",
    "batch_size_one_forward_pass_flops = 2 * combined_df['total_param_count'] + combined_df['layer_size'] * (combined_df['layer_size'] + 1) + combined_df['layer_size'] * (combined_df['layer_size'] - 1) * combined_df['use_resnet'] # param count + multiply scaling + resnet addition WHICH only gets included if use_resnet is true\n",
    "combined_df['total_flops'] = (3 * batch_size_one_forward_pass_flops * combined_df['batch_size'] * combined_df['num_minibatches'] * combined_df['unroll_length'] * combined_df['num_updates_per_batch'] * num_training_steps_per_epoch * combined_df['current_epoch']).astype(int)\n",
    "\n",
    "combined_df['depth'] = combined_df['num_layers'] - 1\n",
    "\n",
    "# combined_df[\"width_depth_ratio\"] = combined_df[\"layer_size\"] / combined_df[\"num_layers\"]\n",
    "combined_df[\"width_depth_ratio\"] = combined_df[\"layer_size\"] / combined_df[\"depth\"]\n",
    "combined_df[\"width_depth_ratio_log2\"] = np.log2(combined_df[\"width_depth_ratio\"])\n",
    "\n",
    "\n",
    "# env_onestep_flop_count_dict= {\"CartpoleSwingup\": 3708.0, \"PandaPickCube\": 172680.0, \"G1JoystickRoughTerrain\": 2497312.0, \"SpotFlatTerrainJoystick\": 116963.0, \"humanoid\": 53748.0, \"humanoid_u_maze\": 1562711.0, \"humanoid_big_maze\": 3264566.0, \"ant_hardest_maze\": 2057127.0, \"ant_u_maze\": 635011.0, \"ant_u4_maze\": 805307.0, \"arm_binpick_hard\": 1942897.0}\n",
    "# env_onestep_flop_count_dict = {\"CartpoleSwingup\": 3712.0, \"PandaPickCube\": 171460.0, \"G1JoystickRoughTerrain\": 2640817.0, \"SpotFlatTerrainJoystick\": 124324.0, \"CartpoleSwingupSparse\": 3654.0, \"CheetahRun\": 46763.0, \"SwimmerSwimmer6\": 21958.0, \"humanoid\": 54779.0, \"humanoid_u_maze\": 1524722.0, \"humanoid_big_maze\": 3435293.0, \"ant_hardest_maze\": 2111963.0, \"ant_u_maze\": 640799.0, \"ant_u4_maze\": 866113.0, \"arm_binpick_hard\": 2003608.0}\n",
    "env_onestep_flop_count_dict = {\"CartpoleSwingup\": 3708.0, \"PandaPickCube\": 172680.0, \"G1JoystickRoughTerrain\": 2476798.0, \"SpotFlatTerrainJoystick\": 119252.0, \"CartpoleSwingupSparse\": 3650.0, \"CheetahRun\": 46293.0, \"SwimmerSwimmer6\": 22583.0, \"AcrobotSwingup\": 4442.0, \"AcrobotSwingupSparse\": 4436.0, \"humanoid\": 53748.0, \"humanoid_u_maze\": 1572027.0, \"humanoid_big_maze\": 3420728.0, \"ant_hardest_maze\": 2083439.0, \"ant_u_maze\": 636961.0, \"ant_u4_maze\": 830291.0, \"arm_binpick_hard\": 1942285.0}\n",
    "\n",
    "combined_df[\"total_env_flops\"] = combined_df[\"total_env_frames\"] * env_onestep_flop_count_dict[env_name]\n",
    "combined_df['flops_model_and_env'] = combined_df['total_flops'] + combined_df[\"total_env_flops\"]\n",
    "\n",
    "\n",
    "# FILTERING COMBINED_DF to match num_envs, unroll_length, num_minibatches, update_epochs, num_timesteps, minibatch_size, base_layer_size\n",
    "# check length before filtering\n",
    "print(f\"Length before filtering: {len(combined_df)}\")\n",
    "combined_df = combined_df[combined_df['num_envs'] == num_envs]\n",
    "combined_df = combined_df[combined_df['unroll_length'] == unroll_length]\n",
    "combined_df = combined_df[combined_df['num_minibatches'] == num_minibatches]\n",
    "combined_df = combined_df[combined_df['num_updates_per_batch'] == update_epochs]\n",
    "combined_df = combined_df[combined_df['num_timesteps'] == num_timesteps]\n",
    "combined_df = combined_df[combined_df['batch_size'] == minibatch_size]\n",
    "combined_df = combined_df[combined_df['base_layer_size'] == base_layer_size]\n",
    "# check length after filtering\n",
    "print(f\"Length after filtering: {len(combined_df)}\")\n",
    "\n",
    "# Create a heatmap to visualize the distribution\n",
    "first_step_df = combined_df[combined_df['step'] == 0]\n",
    "pivot_table = pd.crosstab(first_step_df['layer_size'], first_step_df['num_layers'])\n",
    "# plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_table, annot=True, cmap='rocket_r', fmt='d')\n",
    "plt.title('Distribution of Layer Size vs Number of Layers')\n",
    "plt.xlabel('Number of Layers')\n",
    "plt.ylabel('Layer Size')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25814859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print depths with more than 3 widths\n",
    "layer_size_counts = combined_df.groupby('num_layers')['layer_size'].nunique()\n",
    "Ls = layer_size_counts[layer_size_counts > 2].index.tolist()\n",
    "print(\"Depths with more than 3 unique widths:\", Ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7380ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lrs = [0.01]\n",
    "# Ls = [5,]\n",
    "# Ls = [1,2] # for og cartpole tasks\n",
    "# Ls = [1,2,3,5] # for other tasks\n",
    "# Ns = [4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048]  # for humanoid models\n",
    "Ns = [4, 8, 16, 24, 32, 48, 64, 96, 128, 256, 512, 1024, 2048]  # for everything else\n",
    "\n",
    "# ROLLING_WINDOW = 16\n",
    "ROLLING_WINDOW = 1\n",
    "use_resnet = True\n",
    "# depth_scaling = True\n",
    "\n",
    "\n",
    "params_to_plot = ['NTK', 'μP+1/sqrt(L)',]\n",
    "\n",
    "# bs_str = 'bias_scaling_0'\n",
    "bs_str = 'bias_scaling_0.0001'\n",
    "bias_base_scale = bs_dict[bs_str]['bias_base_scale']\n",
    "bias_init_fn = bs_dict[bs_str]['bias_init_fn']\n",
    "use_bias = bs_dict[bs_str]['use_bias']\n",
    "x_axis = \"total_env_frames\"\n",
    "\n",
    "for base_lr in lrs:\n",
    "    for num_layers in Ls:\n",
    "        print(f\"base_lr: {base_lr}, num_layers: {num_layers}\")\n",
    "        # filter for the following config\n",
    "\n",
    "        # Incrementally merge filtered_df to be bigger and bigger\n",
    "        filtered_df = pd.DataFrame()  # start with empty DataFrame\n",
    "        for param_name, param_dict in parameterization_dict.items():\n",
    "            if param_name not in params_to_plot:\n",
    "                continue\n",
    "            \n",
    "            filter_config = {\n",
    "                \"base_layer_size\": base_layer_size,\n",
    "                \"base_lr\": base_lr,\n",
    "                \"base_output_inverse_scaling\": 1.0,\n",
    "                \"num_layers\": num_layers,\n",
    "                \"init_mode\": \"normal\",\n",
    "                \"activation_str\": \"relu\",\n",
    "                \"use_resnet\": use_resnet,\n",
    "                \"bias_base_scale\": bias_base_scale,\n",
    "                \"bias_init_fn\": bias_init_fn,\n",
    "                \"use_bias\": use_bias,\n",
    "                **param_dict,\n",
    "            }\n",
    "\n",
    "            # grab all rows that match the filter config\n",
    "            current_filtered = combined_df[\n",
    "                combined_df.apply(\n",
    "                    lambda row: all(\n",
    "                        row[key] == value for key, value in filter_config.items()\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            # add a column for the scaling. it will be a combination of ntk_scaling and depth_scaling and how they match up to the parameterization\n",
    "            current_filtered['Scaling'] = param_name\n",
    "\n",
    "            # Incrementally merge into filtered_df\n",
    "            filtered_df = pd.concat([filtered_df, current_filtered], ignore_index=True)\n",
    "\n",
    "        interp_dfs = []\n",
    "\n",
    "        # group the dataframe by specific columns\n",
    "        for (scaling, ntk_scaling, depth_scaling, layer_size, seed), group_df in filtered_df.groupby(\n",
    "            [\"Scaling\", \"ntk_scaling\", \"depth_scaling\", \"layer_size\", \"seed\"]\n",
    "        ):\n",
    "            # TODO: check for duplicates and take average\n",
    "            # print(ntk_scaling, layer_size, run_id)\n",
    "            # sort the group by step to ensure proper ordering\n",
    "            mean_group_df = group_df.groupby('step', as_index=False).mean(numeric_only=True)\n",
    "            # sort the group by step to ensure proper ordering\n",
    "            sorted_group_df = mean_group_df.sort_values(by=\"step\")\n",
    "            # calculate the rolling average on the interpolated values\n",
    "            smoothed_returns = (\n",
    "                sorted_group_df[\"eval/episode_reward\"]\n",
    "                .rolling(window=ROLLING_WINDOW, closed=\"left\")\n",
    "                .mean()\n",
    "            )\n",
    "            # print(smoothed_returns.shape)\n",
    "\n",
    "            temp_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"step\": sorted_group_df[\"step\"],\n",
    "                    x_axis: sorted_group_df[x_axis],\n",
    "                    \"eval/episode_reward\": sorted_group_df[\"eval/episode_reward\"],\n",
    "                    \"smoothed_returns\": smoothed_returns,\n",
    "                    \"ntk_scaling\": ntk_scaling,\n",
    "                    \"layer_size\": layer_size,\n",
    "                    \"seed\": seed,\n",
    "                    \"Scaling\": scaling,\n",
    "                    \"depth_scaling\": depth_scaling,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            interp_dfs.append(temp_df)\n",
    "\n",
    "\n",
    "        if len(interp_dfs) <= 0:\n",
    "            print(f\"No data found for base_lr: {base_lr}, num_layers: {num_layers}\")\n",
    "            continue\n",
    "\n",
    "        interp_df = pd.concat(interp_dfs, ignore_index=True)\n",
    "\n",
    "        # filter out layer size according to values in Ns\n",
    "        interp_df = interp_df[interp_df['layer_size'].isin(Ns)]\n",
    "\n",
    "        # drop the nan values\n",
    "        interp_df = interp_df.dropna(subset=[\"smoothed_returns\"])\n",
    "\n",
    "        # Setup a custom color palette based on the unique values of layer_size.\n",
    "        layer_size_order = sorted(interp_df[\"layer_size\"].unique())\n",
    "        colors = sns.color_palette(\"rocket\", n_colors=len(layer_size_order))\n",
    "        color_mapping = dict(zip(layer_size_order, colors))\n",
    "\n",
    "        # plot formatting\n",
    "        interp_df = interp_df.rename(columns={'layer_size': 'Width'})\n",
    "        \n",
    "        interp_df['Width'] = interp_df['Width'].astype(int)\n",
    "        # plt.rc('text', usetex=True)  # Enable LaTeX rendering\n",
    "        # # You might also need this if you get font warnings:\n",
    "        # plt.rc('font', family='serif')\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(3, 2.5))\n",
    "        ax = sns.lineplot(\n",
    "            data=interp_df,\n",
    "            x=x_axis,\n",
    "            y=\"smoothed_returns\",\n",
    "            hue=\"Width\",\n",
    "            style=\"Scaling\",\n",
    "            palette=color_mapping,\n",
    "            alpha=1,\n",
    "            errorbar=(\n",
    "                \"se\",\n",
    "                1,\n",
    "            ),  # This creates error bands reflecting the confidence intervals over runs.\n",
    "            err_kws={'alpha': 0.1},\n",
    "            dashes=dashes_dict,\n",
    "        )\n",
    "\n",
    "        plt.grid(alpha=0.2)\n",
    "\n",
    "        plt.legend(bbox_to_anchor=(1.03, 1.05), loc=\"upper left\",)\n",
    "\n",
    "        # Add a title and axis labels\n",
    "        plt.title(f\"{env_name}, L={num_layers}, $\\eta_0$={base_lr}, bs: {bias_base_scale}\")\n",
    "        plt.xlabel(\"Env steps ($E$)\")\n",
    "        plt.ylabel(\"Rewards ($G$)\")\n",
    "        filename = f\"widthscale_{env_name}_L{num_layers}_e{base_lr}_bs{bias_base_scale}\"\n",
    "        # plt.savefig(f'{fig_dir}/{filename}.png', bbox_inches=\"tight\",dpi=300)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        print(f\"Saved {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mup-rl-jax-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
